{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61b432e6-e96d-4c20-8fb8-64c08c4c07b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For joining a large DataFrame with a small one, broadcast the smaller one to avoid shuffles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbde291e-2bce-4b23-9a66-b2d9f75a34c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, broadcast\n",
    "\n",
    "# Simulate large and small tables\n",
    "df_large = spark.range(1_000_000).withColumn(\"key\", expr(\"id % 100\"))\n",
    "df_small = spark.range(100).withColumnRenamed(\"id\", \"key\").withColumn(\"value\", expr(\"key * 10\"))\n",
    "\n",
    "# Broadcast the small DataFrame\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "df_joined = df_large.join(broadcast(df_small), on=\"key\", how=\"inner\")\n",
    "df_joined.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74785b6e-644f-46c8-a031-e5992ada7de4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Without Broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55a3964a-a3f6-4d47-8c06-b7e6a2c314b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, broadcast\n",
    "\n",
    "# Simulate large and small tables\n",
    "df_large = spark.range(1_000_000).withColumn(\"key\", expr(\"id % 100\"))\n",
    "df_small = spark.range(100).withColumnRenamed(\"id\", \"key\").withColumn(\"value\", expr(\"key * 10\"))\n",
    "\n",
    "# Broadcast the small DataFrame\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "df_joined = df_large.join(df_small, on=\"key\", how=\"inner\")\n",
    "df_joined.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89c4c960-9c12-463f-a51c-b07069f15f14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Validating Query Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4ec4275-36a6-4208-852e-88eb92005e25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "df_large = spark.range(1_000_000).withColumn(\"key\", (expr(\"id % 100\")))\n",
    "df_small = spark.range(100).withColumnRenamed(\"id\", \"key\")\n",
    "\n",
    "# Without broadcast\n",
    "df_large.join(df_small, \"key\").explain(extended=True)\n",
    "\n",
    "# With broadcast\n",
    "df_large.join(broadcast(df_small), \"key\").explain(extended=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d6bcf06-4b25-45a3-80ca-eec8f1c1a353",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Query Type\tDescription\n",
    "Query A (Implicit)\tRegular join on key, no broadcast hint.\n",
    "Query B (Explicit)\tSame join with BROADCAST() hint on right side.\n",
    "\n",
    "What We Observe from the Physical Plan\n",
    "Both queries used:\n",
    "PhotonBroadcastHashJoin → Broadcast join was applied successfully.\n",
    "\n",
    "BuildRight → The right table (small one) is broadcasted.\n",
    "\n",
    "PhotonRange and PhotonProject → Photon engine is vectorizing every stage.\n",
    "\n",
    "PhotonShuffleExchangeSink → Small dataset is still shuffled, which seems counterintuitive.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "422aabaf-3c36-48c6-8230-40382a62ab9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Why Is There Still a Shuffle?\n",
    "Even though Spark broadcasts the small table:\n",
    "\n",
    "It may use PhotonShuffleExchangeSink to distribute the broadcasted data across nodes during physical planning.\n",
    "\n",
    "This happens especially when the small dataset is partitioned across multiple executors, and Photon parallelizes the broadcast phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54d05c6d-c561-42bb-aef4-a0ae9871a9ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The presence of PhotonShuffleExchangeSink in a broadcast join doesn't mean the large table is shuffled — it's just how Photon handles partitioned inputs efficiently."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4 Broadcast Join Optimization",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
