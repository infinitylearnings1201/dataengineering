{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27d6be8a-ec6a-47a1-a28d-060b386d0e60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "# ðŸ§± Setting Up Product and Customer Dimension Tables in Unity Catalog\n",
    "\n",
    "This setup creates two dimension tables in Unity Catalog under `company.unit`. These tables will enrich transactional data by adding product and customer context â€” a foundational step for building a retail data pipeline.\n",
    "\n",
    "This code sets up two Delta Lake dimension tables â€” product_dim and customer_dim â€” under the Unity Catalog path company.unit. The product_dim table stores product metadata such as ID, category, and price, while the customer_dim table contains customer details like name and region. Sample data is inserted into both tables for enrichment purposes. These tables are foundational for joining with transactional data to support analytics and reporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "447fff02-d1ea-4a6a-a983-db569b3f259e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "-- Use the correct catalog and schema\n",
    "USE CATALOG company;\n",
    "USE SCHEMA unit;\n",
    "\n",
    "-- Product Dimension Table\n",
    "CREATE OR REPLACE TABLE product_dim (\n",
    "  product_id INT,\n",
    "  product_name STRING,\n",
    "  category STRING,\n",
    "  price DOUBLE\n",
    ") USING DELTA;\n",
    "\n",
    "INSERT INTO product_dim VALUES\n",
    "  (101, 'Phone', 'Electronics', 500.0),\n",
    "  (102, 'Laptop', 'Electronics', 1200.0),\n",
    "  (103, 'Shirt', 'Clothing', 40.0),\n",
    "  (104, 'Shoes', 'Clothing', 80.0);\n",
    "\n",
    "-- Customer Dimension Table\n",
    "CREATE OR REPLACE TABLE customer_dim (\n",
    "  customer_id INT,\n",
    "  customer_name STRING,\n",
    "  region STRING\n",
    ") USING DELTA;\n",
    "\n",
    "INSERT INTO customer_dim VALUES\n",
    "  (1, 'Alice', 'North'),\n",
    "  (2, 'Bob', 'West'),\n",
    "  (3, 'Carol', 'East');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c12c64cd-529a-43f5-bb25-d350f9a075dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This code sets the working context to the company catalog and unit schema in Unity Catalog. It then creates a managed volume named sales_input_volume, which is a governed storage location for file-based data. The volume is used to store streaming input files, such as JSON records for sales transactions. This enables secure, trackable ingestion into Databricks pipelines using Autoloader or file-based reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71103060-44b6-47e1-802d-a67043a020cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "USE CATALOG company;\n",
    "USE SCHEMA unit;\n",
    "\n",
    "CREATE VOLUME IF NOT EXISTS sales_input_volume\n",
    "COMMENT 'Volume for streaming sales input JSON files';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7c427e0-b754-49c3-834d-f7a3e48c1279",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This command creates (or replaces) a Delta Lake table named sales_raw under the current Unity Catalog catalog and schema. The table is designed to store raw sales transaction data with fields like timestamp, transaction ID, customer ID, product ID, and quantity. It acts as a bronze layer table in the data pipeline, typically populated using streaming ingestion from files stored in a volume. This table serves as the starting point for further enrichment and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5731c17-7138-41d9-942b-cb97ff9f2f91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "CREATE OR REPLACE TABLE sales_raw (\n",
    "  transaction_time TIMESTAMP,\n",
    "  transaction_id INT,\n",
    "  customer_id INT,\n",
    "  product_id INT,\n",
    "  quantity INT\n",
    ") USING DELTA;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a31e2b7c-432c-4f7a-bb71-d66ac506f741",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This code sets up a streaming DataFrame using Databricks Autoloader (cloudFiles) to ingest JSON files from the Unity Catalog volume path /Volumes/company/unit/sales_input_volume/. It defines the schema for incoming sales transaction data with fields like timestamp, transaction ID, customer ID, product ID, and quantity. The data is read as a real-time stream, enabling continuous or micro-batch processing for downstream analytics and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22c44d02-81b2-45b8-af4b-1c67e7355818",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "from pyspark.sql.functions import col, window, expr, sum as spark_sum, count\n",
    "\n",
    "sales_stream_df = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .schema(\"transaction_time TIMESTAMP, transaction_id INT, customer_id INT, product_id INT, quantity INT\")\n",
    "    .load(\"/Volumes/company/unit/sales_input_volume/\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2364d731-072b-4621-9405-9c109af40481",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This code reads static dimension tables product_dim and customer_dim from Unity Catalog and joins them with the streaming sales data (sales_stream_df). It enriches each transaction with product and customer details. A new column spend is computed by multiplying quantity and price, representing the total transaction value. This prepares the enriched dataset for downstream analytics like customer spend tracking or category-level aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79e92c78-bbe6-41d4-ad9f-7affc3e92a9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "product_df = spark.read.table(\"company.unit.product_dim\")\n",
    "customer_df = spark.read.table(\"company.unit.customer_dim\")\n",
    "\n",
    "enriched_sales = sales_stream_df \\\n",
    "    .join(product_df, \"product_id\", \"left\") \\\n",
    "    .join(customer_df, \"customer_id\", \"left\") \\\n",
    "    .withColumn(\"spend\", col(\"quantity\") * col(\"price\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bd69b1b-0f78-46e7-b34b-337ee8bcfcd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This code sets the active catalog to company and schema to unit within Unity Catalog. It then creates a managed volume named chk, which will be used to store checkpoint data for streaming queries. Checkpoints are essential for maintaining streaming state and ensuring exactly-once processing. This setup allows Databricks Structured Streaming to recover from failures and resume from the last successful state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec926326-b198-417b-bc12-3fd07bd7b2d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "USE CATALOG company;\n",
    "USE SCHEMA unit;\n",
    "\n",
    "CREATE VOLUME IF NOT EXISTS chk\n",
    "COMMENT 'Volume for streaming sales input JSON files';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b72a45d9-a942-4f02-83b9-910c9db7eeb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This code calculates lifetime spending per customer by aggregating the spend column from the enriched sales stream. It writes the aggregated result to a Delta table named company.unit.customer_spending. The stream uses trigger(availableNow=True), which processes all available data once and then stops. A checkpoint is stored in the chk volume to ensure recoverability and incremental processing in future runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f407634-86ea-4ea8-a217-cfc34f598f07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "customer_spending = enriched_sales.groupBy(\"customer_id\").agg(\n",
    "    spark_sum(\"spend\").alias(\"lifetime_spent\")\n",
    ")\n",
    "\n",
    "customer_spending.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .option(\"checkpointLocation\", \"/Volumes/company/unit/chk/customer_spending\") \\\n",
    "    .toTable(\"company.unit.customer_spending\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc11e853-8f10-4840-adf2-57c3bb553aa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Simulate\n",
    "\n",
    "This code uses dbutils.fs.put() to write a sample JSON file named sample.json into the Unity Catalog volume sales_input_volume. The file contains a single sales transaction record with fields like timestamp, customer ID, product ID, and quantity. This JSON file simulates streaming input data for testing the Autoloader pipeline. The overwrite=True flag ensures the file is replaced if it already exists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d653aa2-16f8-4356-8c3d-5581d75c5b0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "dbutils.fs.put(\"/Volumes/company/unit/sales_input_volume/sample.json\", \"\"\"\n",
    "{\n",
    "  \"transaction_time\": \"2024-06-06T10:00:00\",\n",
    "  \"transaction_id\": 1001,\n",
    "  \"customer_id\": 1,\n",
    "  \"product_id\": 101,\n",
    "  \"quantity\": 1\n",
    "}\n",
    "\"\"\", overwrite=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89aa0f2e-9692-4a52-b28f-e96edcaf5408",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faf5aab9-73d7-42b6-92b5-7efc2fe55ad4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select * from company.unit.customer_spending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edb1a9da-6b4e-44f8-a146-3b656af2bf01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#Adding row\n",
    "\n",
    "This command writes another sample JSON file named sample1.json into the Unity Catalog volume sales_input_volume. It represents a sales transaction where customer 2 purchases 3 units of product 102 at a specific timestamp. This file serves as additional input data for the streaming pipeline. The overwrite=True flag ensures the file is replaced if it already exists, supporting repeated testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa17b3af-da23-47f0-a5f1-5579613d67df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "dbutils.fs.put(\"/Volumes/company/unit/sales_input_volume/sample1.json\", \"\"\"\n",
    "{\n",
    "  \"transaction_time\": \"2024-06-06T10:00:00\",\n",
    "  \"transaction_id\": 1002,\n",
    "  \"customer_id\": 2,\n",
    "  \"product_id\": 102,\n",
    "  \"quantity\": 3\n",
    "}\n",
    "\"\"\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce6be3e4-fb7b-453b-816c-feb23a4ca94f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Second Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b181a769-9e37-4801-a347-c7a057b5acd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Test and Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c819fd2-9752-4908-b1ad-03dfb4c37e21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "customer_spending = enriched_sales.groupBy(\"customer_id\").agg(\n",
    "    spark_sum(\"spend\").alias(\"lifetime_spent\")\n",
    ")\n",
    "\n",
    "customer_spending.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .option(\"checkpointLocation\", \"/Volumes/company/unit/chk/customer_spending\") \\\n",
    "    .toTable(\"company.unit.customer_spending\")\n",
    "\n",
    "    %sql\n",
    "\n",
    "select * from company.unit.customer_spending"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Day 6 Capstone Project",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
